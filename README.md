# Generating_Rationale_in_Visual_Question_Answering

The objective of this task is to predict an answer and a rationale behind that answer given an image, a question and a set of multiple choice options. In the Computer Vision literature, the task of choosing the right answer from a set of options, given a question and an image, is know as Visual Question Answering (VQA). There has been some current work on trying to choose the rationale behind choosing an answer given a set of rationales to choose from. The novelty of our task lies in the fact that it further asks the model to provide a rational to justify the answer it predicted. This makes the task more interesting and concurrently more difficult. We build on this task to try to generate the rationale instead of choosing the rationale from a given set of options.
 
Existing approaches don't require the model to consider rationale or reasons while predicting answer, which defeats the purpose of the task. We propose to generate rationales from the predicted answer as a means to investigate the understanding of the model while predicting answer. Further, by training the model jointly for answer prediction and rationale generation, we seek to force the model to consider rationales while predicting answers. 

The problem of commonsense reasoning is a difficult task in itself. To generate reason/rationale makes it even more difficult. Very limited work have addressed this task in text only data. And as far as we know, no one has tried to generate rationales from visual data. This is especially non-trivial because the model not only has to understand visual data, but also express rationales in text mode.

We use the state-of-the-art pretrained model "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", a model for learning task-agnostic joint representations of image content and natural language. And we use GPT-2 for rationale genration.

We run 2 sets of experiments. In the first experiment, we freeze the weights of the ViLBERT model and just train the rationale generation component of our architecture. In the second, experiment, we perform an end-to-end training of our model which updates the parameters of the ViLBERT model. As expected, our second experiment outperforms the first because the parameters of the ViLBERT model have not been trained explicitly to do rationale generation. Therefore, in the second experiment when we update the parameters of the ViLBERT model for the task of rationale generation, it outperforms the original ViLBERT model.

We evaluate the performance of our proposed architecture using some standard performance metrics, like the SPICE, ROGUE and the BLEU score. The dataset can be found at (https://visualcommonsense.com/). I encourage you to read the report, the presentation, the ViLBERT paper, and the Visual Commonsense Reasoning paper to better understand how it works and how to reproduce it.
